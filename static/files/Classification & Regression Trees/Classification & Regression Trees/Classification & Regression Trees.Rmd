---
title: "Classification & Regression Trees"
date: "Last updated on `r Sys.Date()`"
author: 'Lauren Forando'
output:
  html_document: 
    code_folding: show
    df_print: kable
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
---

```{r setup, include=FALSE}

# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE, 
  fig.width = 16/2, 
  fig.height = 9/2
)

# Load all your used packages here:
library(tidyverse)
library(scales)
library(rpart)
library(plyr)

# Set seed value of random number generator here:
set.seed(76)

# Load data
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")

```



***



# Modeling

## Model fit

Fit model using CART here & save it in `SalePrice_CART_model`. 

```{r, include = FALSE}
set.seed(76)
train_validation <- training %>%
 sample_frac(0.5)
test_validation <- training %>%
 anti_join(train_validation, by = "Id")

test <- test %>%
  mutate(TotalBsmtSF = ifelse(is.na(TotalBsmtSF),0,TotalBsmtSF))  %>%
  mutate(BsmtUnfSF = ifelse(is.na(BsmtUnfSF),567.2,BsmtUnfSF))

sapply(lapply(na.omit(test)[sapply(na.omit(test), is.factor)], droplevels), nlevels)
```


```{r}
SalePrice_CART_model2 <- rpart(log(SalePrice+1) ~ BsmtUnfSF + OverallQual + GrLivArea + TotalBsmtSF + YearRemodAdd + GarageArea + factor(Neighborhood) + factor(ExterQual) + factor(HouseStyle) + factor(SaleCondition) + (MSSubClass) + KitchenQual, data = train_validation, control = rpart.control(cp = 0))

y_hat <- predict(SalePrice_CART_model2, type="vector", newdata = test_validation)

test_validation %>% 
  mutate(
    SalePrice_hat = exp(y_hat),
    le = log(SalePrice + 1) - log(SalePrice_hat + 1),
    sle = le^2) %>% 
  summarize(msle = mean(sle)) %>% 
  mutate(rmsle = sqrt(msle))
```

***



# Choice of complexity parameter `cp`

Write down all the `cp` values you tried and why/how you chose these values.

* First tried `cp = 0` because it was simple, although somewhat arbitrary.
* Then then tried `cp = 0.2` somewhat arbitrarily, and continued with `cp = 0` because the score was better.
* The ultimate Kaggle submission used was`cp = 0`, which was chosen because nothing with a nonzero cp had a lower rmsle than the zero cp. 



***



# Kaggle score

## Create submission CSV

```{r}
sub_predicted <- predict(SalePrice_CART_model2, newdata = test)

submission <- test %>%
  select(Id) %>%
  mutate(SalePrice = exp(sub_predicted)-1)

#submission$SalePrice <- exp(sub_predicted)
write_csv(submission, path = "data/submission.csv")
```


## Comparison of multiple regression and CART model Kaggle scores

Compare your Kaggle scores from the multiple regression problem set with your new CART Kaggle score in a cleanly formatted table. Which model had the best score? Make your best guess as to why.


![](images/score_screenshot_LaurenF.png){ width=100%}

```{r}
tb <- matrix(c("LaurenF Multiple Regression",0.17887,"CART Model", 0.20373),ncol=2,byrow=TRUE)
colnames(tb) <- c("Name","Kaggle Score")
rownames(tb) <- c(1,2)
tb <- as.table(tb)
tb
```

The multiple regression model seemed to be better than the CART model. From some searching on the internet it seems that linear regression works better than decision trees on data that can be closely fit to a linear correlation, such as the house pricing data once it was log transformed. CART is said to work better for data where the correlation is much less linear. 
